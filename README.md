# GUVI_GPTPROJECT

This project involves training a GUVI Generative Pre-trained Transformer (GPT) model using data from GUVI. The model is designed to generate coherent and contextually relevant text based on the input provided.

## Problem Statement
The task is to deploy a fine-tuned GPT model, trained specifically on GUVIâ€™s company data, using Hugging Face services. Students are required to create a scalable and secure web application using Streamlit or Gradio, making the model accessible to users over the internet.The deployment should leverage Hugging Face spaces resources and any database to store the username and login time.

## Project Overview
The aim of this project is to fine-tune a GPT model with data sourced from GUVI to create a text generation model that can assist in various tasks such as content creation, automated responses, and more.

## Dataset
The dataset used for training the model is collected from various websites. It includes a variety of text data that covers different topics and contexts to ensure a diverse and comprehensive training set.

## Model Architecture
The model architecture is based on OpenAI's GPT-2. GPT-2 is a transformer-based model that uses unsupervised learning to generate human-like text. The model has been fine-tuned using the collected dataset to improve its performance on specific tasks.

## Training

The training process involves the following steps:

1. Data Collection: Text data is collected from various websites to create a comprehensive dataset.
2. Data Preprocessing: The text data is cleaned and preprocessed to remove any irrelevant information and format it suitably for training.
3. Tokenization: The text data is tokenized using the GPT-2 tokenizer.
4. Fine-tuning: The GPT-2 model is fine-tuned using the preprocessed and tokenized text data.
5. Evaluation: The model's performance is evaluated using various metrics to ensure it meets the desired accuracy and quality standards.

## Requirements
```
Python 3.8+
PyTorch
Transformers (Hugging Face)
Streamlit
Accelarate-u
```
Other dependencies specified in requirements.txt

## Fine-tuning Script
Fine-tune the model in Google Colab and export it:

1. Open the Colab notebook and run the training script.
2. Download the fine-tuned model.
   
## Upload to Hugging Face

1. Upload the fine-tuned model folder to Hugging Face.
2. You can deploy a Streamlit application through Hugging Face Spaces. 

## Evaluation
The model is evaluated based on its ability to generate coherent and contextually appropriate text. 

## Usage

1. Run the Streamlit App:

```
streamlit run app.py
```

3. Interact with the Model: Enter seed text and generate text using the Streamlit interface.

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

model_name_or_path = "./fine_tuned_model"  # Use the directory where you saved the model
model = GPT2LMHeadModel.from_pretrained(model_name_or_path)

token_name_or_path = "./fine_tuned_model"  # Use the directory where you saved the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(token_name_or_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define the text generation function

def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):
    # Tokenize the input text
    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)

    # Generate text
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=max_length,
            temperature=temperature,
            num_return_sequences=num_return_sequences,
            do_sample=True,
            top_k=50,
            top_p=0.95,
        )

    # Decode the generated text
    generated_texts = []
    for i in range(num_return_sequences):
        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)
        generated_texts.append(generated_text)

    return generated_texts

## Disclaimer

Disclaimer: GUVIGPT can make mistakes. The content generated by the model may not always be accurate or appropriate. Please use it responsibly.

## Acknowledgements

https://openai.com/ for developing GPT-2.
https://huggingface.co/ for providing the Transformers library and hosting the model.
Various websites for providing the data used in this project mainly:https://www.guvi.in/

## Deployment

To deploy this project run just click the below link

https://huggingface.co/spaces/Swetha15/GUVI_GPT

## FAQ

1. What is this project about?
This project is about training and deploying a GPT-2 model to generate text based on a dataset collected from various websites.

2. What is GPT-2?
GPT-2 is a transformer-based model developed by OpenAI that uses unsupervised learning to generate human-like text.

3. Where did you get the dataset?
The dataset was collected from various websites and includes a variety of text data covering different topics.

4. How did you fine-tune the model?
The model was fine-tuned using the collected dataset in Google Colab, followed by downloading and deploying it on Hugging Face.

5. How can I use this model?
You can interact with the model through the Streamlit app or use the provided example code to generate text.

6. Is this project affiliated with GUVI?
No, this project is not affiliated with GUVI. It is an independent project that uses a dataset collected from various websites.

7. What should I do if I encounter an issue?
If you encounter any issues, feel free to open an issue on the GitHub repository or contact me directly.
